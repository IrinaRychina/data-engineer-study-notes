> source https://github.com/halltape/HalltapeRoadmapDE/blob/main/QUESTION/README.md
### Вопросы по Spark(PySpark).

* Что такое Spark?

_(первый ответ из головы)_
Это фреймворк, который нужен для обработки данных.

_(исправленный ответ после повторения теории)_
Это фреймворк, который нужен для распределенной обработки больших данных.

* Объясни парадигму MapReduce и почему Spark пришел ей на замену?

_(первый ответ из головы)_
парадигма MapReduce - это принцип распределенной обработки больших данных. В нем данные делятся на партиции и обрабатываются параллельно на разных воркерах, а после завершения обработки собираются с помощью shuffle в один результирующий файл.

Spark выполняют ту же задачу, но имеет более совершенный механизм, из-за которого скорость обработки выше в несколько раз, чем в MapReduce.


_(исправленный ответ после повторения теории)_

* Что делает Shuffle в Spark? Между чем передаются данные?

* Как передать UDF?

* Какие типы трансформаций бывают?

* Какие проблемы могут быть с shuffle?

_(первый ответ из головы)_
если файл разбит на слишком много партиций, то может быть проблема слишком большого количества передачи данных по сети.

_(исправленный ответ после повторения теории)_

* Что такое spill? И в чем их причины? Какие варианты решения проблем со spill-файлами?

* Что такое data skewing? Как можно решить данную проблему?

* В чем различие coalesce и repartition?

* Чем отличается RDD от DataFrame?

* Для чего в Spark используется cache?

* Почему нельзя использовать Pandas для больших данных, а нужно использовать Spark?

_(первый ответ из головы)_
потому что Pandas может работать только на одном сервере, не умеет делать распределенную между серверами обработку, из-за этого если объем данных слишком большой, то когда они все будут выгружены в оперативную память для обработки, то ресурсов сервера может не хватить.
Поэтому для таких задач больше подходит spark, он умеет распределять нагрузку между несколькими серверами.

* Минимальное параллелизм в Spark и что это такое?

* Что такое RDD в Spark?

* Что такое Dataset и чем отличается от dataframe и RDD?

* Какие виды кэширования существуют в Spark и чем они отличаются?

* Что такое persist в Spark и какие storage levels существуют?

_(первый ответ из головы)_
Это команда, которая позволяет сохранить вычисленные данные.
От выбора storage level будет зависеть, где именно эти данные будут сохранены.
storage levels: IN_MEMORY, ON_DISK, MEMORY_AND_DISK, IN_MEMORY_SER, ON_DISK_SER, OFF_HEAP

* Какие настройки Spark applications вы используете?

* Что такое broadcast join в Spark и как его настроить?

* Что такое ленивые вычисления в Spark?
это парадигма, которая позволяет вычислять выраженя, только когда они действительно нужны.
Например, выражения select, filter, groupBy будут вычичслены только тогда, когда встретится команда, для которой результат нужно будет показать/сохранить.

* Что такое Adaptive query execution?
